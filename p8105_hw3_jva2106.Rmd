---
title: "Homework 3"
author: "Jyoti Ankam"
date: "October 7, 2018"
output: github_document
---

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

Loading the tidyverse dataset and other relevant datasets-

```{r}
library(tidyverse)

library(p8105.datasets)

library(ggthemes)

library(patchwork)

library(hexbin)
  
```

Formatting the data to use appropriate variable names with a focus on the “Overall Health” topic and including only responses from “Excellent” to “Poor” while organizing responses as a factor taking levels from “Excellent” to “Poor”:

```{r}
data("brfss_smart2010")
brfss_smart_df = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  select(-(class:question), -sample_size, -(confidence_limit_low:geo_location)) %>% 
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) 
 
```

States that were observed at 7 locations are -

```{r}
brfss_smart_df %>% 
  filter(year == 2002) %>% 
  group_by(locationdesc) %>%
  distinct(locationdesc) %>% 
  summarise(n = n()) %>% 
  filter(n == 7) %>% 
  knitr::kable()

```

The states that were observed at 7 locations in 2002 are NC, CT and FL

```{r}
brfss_smart_df %>%
  distinct(locationabbr, locationdesc, year) %>% 
  group_by(locationabbr, year) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) +
  geom_line() +
  theme(legend.position = "left") +
  labs(
    title = "Spaghetti plot",
    x = "Year",
    y = "Number of observed locations",
    caption = "BRFSS data - Year 2010"
  )
```

Making a “spaghetti plot” showing the number of locations in each state from 2002 to 2010. As we can see there was a sudden increase in the number of locations for one of the states. The number of locations for majority of the other states, however, remained stable.

Making a table showing the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State for the years 2002, 2006, and 2010.

```{r}
brfss_smart_df %>% 
  filter(year %in% c(2002, 2006, 2010) & locationabbr == "NY") %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(locationabbr, year) %>% 
  summarise(n = n(),
            mean = mean(excellent),
            sd = sd(excellent)) %>% 
  knitr::kable()
 
```

From the table, we can observe that the mean for proportion of excellent for the years 2006 and 2010 are very similar but worse than 2002. The standard deviation are not very different for 2002 and 2006.

In order to make the five panel plot showing the each of the response categories separately, we can first compute the average proportion in each response category for each year and state and then we can compute by taking the average across locations in a state. This way we can see the distribution of these state-level averages over time.

```{r}
brfss_smart_df %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(locationabbr, year) %>% 
  summarise(n = n(),
            mean_excellent = mean(excellent, na.rm = TRUE),
            mean_very_good = mean(very_good, na.rm = TRUE),
            mean_good = mean(good, na.rm = TRUE),
            mean_fair = mean(fair, na.rm = TRUE),
            mean_poor = mean(poor, na.rm = TRUE)) %>%
  gather(key = mean_name, value = mean_value, mean_excellent:mean_poor) %>% 
  ggplot(aes(x = year, y = mean_value, color = locationabbr)) +
  geom_line() +
  facet_grid(~mean_name)
```
In this spagetti plot, we can observe that across the years, the proportion of very good has the highest mean while the lowest mean is for the proportion of the poor.

Problem 2:

Loading the dataset "instacart":

```{r}

data("instacart")

```

Describing the dataset:

This dataset contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. Variable 'order_id' is a unique identifier of the orders that have been made. Also, we can observer that there are`r nrow(distinct(instacart, order_id))` distinct orders. Other variables like 'aisle', 'product_name', and 'department_name', have their own unique identifiers as well. Variables such as 'order_dow' tell us about the day of the week the order was made and 'order_hour_of_day' tell us the time the order was made. In addition, we can derive information from the `r nrow(distinct(instacard, user_id))` unique customers.

Calculating the number of aisles, and also which aisles are the ones with the most items ordered from:

```{r}

instacart %>% 
  distinct(aisle_id) %>% 
  count()

instacart %>%
  group_by(aisle_id, aisle) %>% 
  summarise(number = n()) %>% 
  ungroup() %>% 
  top_n(5, number) %>% 
  arrange(desc(number))
  knitr::kable()

```
Here, we can see that there are `r pull(count(distinct(instacart, aisle_id)))` aisles. Also, we get the top five most popular aisles with fresh vegetables the most popular.

Below is the plot showing the number of items ordered in each aisle. 

```{r}

instacart %>%
  group_by(aisle_id, aisle) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number)) %>%
  mutate(aisle = tools::toTitleCase(aisle)) %>% 
  ggplot(aes(x = reorder(aisle, -number), y = number, fill = aisle)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 85, hjust = 1, size = 8), 
        legend.position = "none") +
  labs(
    title = "Frequency distribution of aisle orders",
    x = "Aisle",
    y = "Number of orders",
    caption = "Instacart Data 2017"
  )

```

As evident from the plot, the aisle containing fresh vegetables is where the most orders come from.

Below is the table showing the most popular item in aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}
instacart %>% 
  select(product_name, aisle) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>% 
  summarise(number = n()) %>% 
  top_n(1, number) %>%
  arrange(desc(number))
  knitr::kable()

  
```

Below is the table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r}

instacart %>% 
  filter(product_name == "Pink Lady Apples") %>% 
  group_by(order_dow, product_name) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = product_name, value = mean_hour)
  knitr::kable()

instacart %>% 
  filter(product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow, product_name) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = product_name, value = mean_hour)
  knitr::kable()

```

Problem 3:
Installing the dataset -

```{r}

data("ny_noaa")

```

Doing some data cleaning and creating separate variables for year, month, and day. Ensuring that the observations for temperature, precipitation, and snowfall are given in reasonable units (i.e. tenths of their units) by dividing by 10. For snowfall, calculating the most commonly observed values.

```{r}
ny_noaa_df = ny_noaa %>% 
  mutate(month = months.Date(date),
         day = chron::days(date),
         year = chron::years(date),
         tmax = (as.numeric(tmax))/10,
         tmin = (as.numeric(tmin))/10,
         prcp = prcp/10)

```

This dataset contains `r nrow(ny_noaa_df)` observations and `r ncol(ny_noaa_df)` variables. There are variables decribing precipitation (prcp), maximum and minimum temperature (tmax and tmin), snow fall(snow) and snow depth (snwd). We can see `r nrow(distinct(ny_noaa_df, id))` distinct stations recording all the various weather-related information from 1981 to 2010.

```{r}

ny_noaa_df %>% 
  group_by(snow) %>% 
  summarise(number = n()) %>% 
  arrange(desc(number))

```

As we can observe for snowfall, the most common observed values are 0 and NA. This is an obvious observation as it does not snow on most days in a year. Also, there might be stations in geographic locations having no snow fall and hence with no recordings.

Making a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r}

ny_noaa_df %>% 
  group_by(id, year, month) %>% 
  summarize(n = n(),
            sum = sum(tmax, na.rm = TRUE),
            mean = mean(tmax, na.rm = TRUE)) %>% 
  filter(month %in% c("January", "July")) %>% 
  ggplot(aes(x = year, y = mean, color = id)) +
  facet_grid(~month) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 85, hjust = 1, size = 8),
        legend.position = "none") +
  viridis::scale_color_viridis(
    name = "location", 
    discrete = TRUE) 

```
It is evident that there are higher temperatures in July as compared to January, as expected. There are a a couple of outliers in some of the years.

Making a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) making a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}

hex = ggplot(ny_noaa_df, aes(x = tmin, y = tmax, color = id)) +
  geom_hex(aes()) +
  theme(axis.text.x = element_text(angle = 85, hjust = 1, size = 8),
      legend.position = "none")

my_boxplot = ny_noaa_df %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) +
  geom_boxplot(alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 85, hjust = 1, size = 8),
        legend.position = "none")

hex / my_boxplot

```

The hexagonal heat map shows the distribution of the pairs of tmax and tmin. From the box plot, we can observe that there is more variability in snowfall in the last ten years compared to first ten years. Additionally, there are more outliers in the last ten years compared to the first ten years.